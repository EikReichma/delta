{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyuoS1V_A74Q"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/demos/nlp/nlp_foundations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skvR6YA3A74T"
   },
   "source": [
    "# Fundamentals of natural language processing (NLP)\n",
    "The notebook revisits the lecture on the foundations of natural language processing. We examine common tasks in the preparation of textual data for analysis. Several Python libraries including `scikit-learn` and `Keras` offer functionality for text data prepartion. In this notebook, we will use the `NLTK toolkit`. It has a clear and easy to understand syntax and is well-suited to demonstrate standard NLP operations. Although not the focus of this tutorial, we also introduce a library called `Beautiful Soup`, which gained a lot of popularity in web-scraping. Make sure to have these libraries installed before running the following codes. \n",
    "\n",
    "Also note that the demo draws inspiration from a [Kaggle kernel](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook#Conversion-of-Emoji-to-Words). The kernel demonstrates yet more functionality s check it out if you are interested.\n",
    "\n",
    "\n",
    "Here is the agenda of the session:\n",
    "\n",
    "1. Preparing text for analysis: the standard NLP pipeline\n",
    "2. Use case: the IMDB movie review data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1665573853100,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "hB9cN44lA74V",
    "outputId": "accb314b-cca8-4eee-aa11-c54417941e45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Library for standard NLP workflow\n",
    "import nltk  \n",
    "# When running this notebook for the first time, you have to download some NLTK packages. To do so, simply uncomment the next lines\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft9tayXsA74X"
   },
   "source": [
    "### 1. Preparing text for analysis: the standard NLP pipeline\n",
    "To illustrate standard NLP preprocessing operations, we need some demo text. Below is an extract from a famous book; no need to quote I guess 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1665573853665,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "0JQocRpsA74Z",
    "outputId": "5f76aec8-a122-4ec5-b5f3-f378276cc3c7",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "            I wonder if I have been changed in the night. Let me think. Was I the same when I got up this morning? \n",
      "            I almost can remember feeling a little different. But if I am not the same, the next question is 'Who in the world am I?' \n",
      "            Ah, that is the great puzzle!\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "text_raw = \"\"\" \n",
    "            I wonder if I have been changed in the night. Let me think. Was I the same when I got up this morning? \n",
    "            I almost can remember feeling a little different. But if I am not the same, the next question is 'Who in the world am I?' \n",
    "            Ah, that is the great puzzle!\n",
    "           \"\"\"\n",
    "print(text_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaw7ErgNst0W"
   },
   "source": [
    "In the following parts, we incrementally build the functionality for a full preprocessing chain. To be able to nicely put everything together in the end, we will wrap up every piece of functionality that we build in a custom function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WglFCITTTmXk"
   },
   "source": [
    "#### Remove Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1665573853666,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "PTrQd1jjTqd6",
    "outputId": "0e6a297b-0c9d-4ca4-d814-39042d9683ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wonder if I have been changed in the night. Let me think. Was I the same when I got up this morning? I almost can remember feeling a little different. But if I am not the same, the next question is 'Who in the world am I?' Ah, that is the great puzzle!\n"
     ]
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" Function to remove whitespace (tabs, newlines). \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "text_processed = remove_whitespace(text_raw)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ZFX6nlT5q9"
   },
   "source": [
    "Hm, but the punctuation is there still. Is it noise or is it useful? Let's try removing it for now (there is a bunch of methods out there). Additionally we will drop weird symbols and lower the big cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUPvGuL2A74g"
   },
   "source": [
    "#### Punctuation, Whitespace and Casing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1665573853666,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "xX10anQDEYlm",
    "outputId": "f163dab5-df10-4373-f3a4-bf8886a10e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wonder if i have been changed in the night let me think was i the same when i got up this morning i almost can remember feeling a little different but if i am not the same the next question is in the world am i ah that is the great puzzle\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_and_casing(text):\n",
    "    \"\"\"\n",
    "    Function to remove the punctuation, upper casing and words that include\n",
    "    non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    chars = '!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    text = text.translate(str.maketrans(chars, ' ' * len(chars)))\n",
    "    return ' '.join([word.lower() for word in text.split() if word.isalpha()])\n",
    "\n",
    "text_processed = remove_punctuation_and_casing(text_processed)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOaPEAuCA74i"
   },
   "source": [
    "This is starting to look like a bag of words, right? There are some more issues we want to address though. Like 'stop words' - semantically they do not mean much but serve to put sentences together (\"the\", \"a\", \"and\", etc) - they will add noise. NLTK can offer you its own list of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mZqEl0NA74k"
   },
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1665573853666,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "wJ0OQxuPMz3i",
    "outputId": "023ee7b0-c79f-4256-ba3d-7283ec76d6a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1665573853667,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "Iv4tYmvGNlKh",
    "outputId": "c7263163-f028-47eb-fab7-0a69af506145"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG8Rf-TFA74n"
   },
   "source": [
    "The list of stop words looks comprehensive. However, say you miss a 'stop word' that you would also like to filter. You can extend the above list easily. After all, it is just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1665573853670,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "y58lHR8HA74p",
    "outputId": "944f7efd-c106-4b6e-aa27-18ff7bd5945a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of stopwords is: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "t = type(english_stopwords)\n",
    "print('Data type of stopwords is:', t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1665573853670,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "WMAbMUW0A74q",
    "outputId": "72a03133-a6d3-4478-e6d6-be37db85a796"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some_word_you_dont_like'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add some custom stopwords\n",
    "english_stopwords.append('some_word_you_dont_like')  # you can do everything that is allowed with a <list>\n",
    "english_stopwords[-1]  # for example get the element at a certain position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZJ3fUEPNt_q"
   },
   "source": [
    "Finally, let's remove the stopwords from our processed sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1665573853671,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "D775Y7WhNegS",
    "outputId": "5588395d-f14d-488c-f22f-f61f8c997fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonder changed night let think got morning almost remember feeling little different next question world ah great puzzle\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" Function to remove stopwords. \"\"\"\n",
    "    return ' '.join([word for word in str(text).split() if word not in english_stopwords])\n",
    "\n",
    "text_processed = remove_stopwords(text_processed)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb0EsGmQst0Y"
   },
   "source": [
    "Quite a reduction in the number of tokens by filtering stopwords, correct? Recall that the number of unique tokens plays a key role in the bag of word model. For example, representing text in the form of a document term matrix (DTM), the dimensionality of the DTM will be equal to the number of distinct tokens. So let's try to reduce it even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ON3INRoA74v"
   },
   "source": [
    "#### Lemmatization and Stemming\n",
    "You might have already thought of the issue: what if a word is used in different forms? It will be treated as different words semantically right? That is where **stemming** and **lemmatization** comes into play. The former approach is simpler and consists mainly of 'cutting of the end' of words. The later reduces a word to its dictionary form. To that end, we need to have a dictionary available. Let's first illustrate simple stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1665573853671,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "mO2xMJs-OUAn",
    "outputId": "a8c4dd3f-b653-47fb-d32f-68df1f669000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonder chang night let think got morn almost rememb feel littl differ next question world ah great puzzl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  # Other stemmers are supported as well\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\" Function to stem words. \"\"\"\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "text_processed = stem_words(text_processed)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuHgXVBOA74y"
   },
   "source": [
    "Simple, isn't it. With just one example sentence, it is hard to appreciate the benefits of stemming. The idea is that if we have a large corpus many words will appear multiple times in different grammatical forms. Still, the meaning that these words carry is roughly identical. Running, run, ran, runner, etc. all of these words indicate that the text has something to do with running. Assuming that this is all we need to know -- yes that is a bold assumption -- stemming makes sense as it could greatly reduce the number of distinct words in a corpus. This number of distinct words, also called **vocabulary size**, is very important. It effects the efficiency of NLP operations and may also have a big impact on the accuracy of text classification. <br>\n",
    "Let's now take a look on lemmatization. Here, things are a little more complicated. While NLTK offers a ready-to-use function, we need to tell it the grammatical form of the word that we want to lemmatize. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1665573854903,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "SQhYXvf1A741",
    "outputId": "e0fd39eb-d11e-4df8-cd49-f5429aa2ada4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripe\n",
      "strip\n"
     ]
    }
   ],
   "source": [
    "# NLTK lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# You need to choose the type of word:\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  # here we claim stripes is a noun\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))  # what happens if we claim it is a verb? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR5BVbzeA743"
   },
   "source": [
    "How would we know that grammatical form? In fact, determining this form is an NLP task in its own right. It is called **POS tagging**. Much research has been done on coming up with clever ways to determine POS (part-of-speech) tags. We will not go into details. A simple POS tagger is available as part of the `NLTK` library. It can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1665573854904,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "MZ8B969jA743",
    "outputId": "a9c9a171-5c5d-43ae-90ce-cfc1b61675c7",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('earned', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('stripes', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('great', 'JJ'),\n",
       " ('performance', 'NN')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"She\", \"earned\", \"her\", \"stripes\", \"with\", \"great\", \"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0YTMpiOA744"
   },
   "source": [
    "We make use of the above POS tagger later. For now, let's simply use the lemmatizer to reduce words to their dictionary form, irrespective of part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1665573854905,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "O8mb4ZdDPnO0",
    "outputId": "5066060f-b3cf-409e-b740-fc987950b4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonder chang night let think got morn almost rememb feel littl differ next question world ah great puzzl\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_words(text, **kwargs):\n",
    "    \"\"\" Function to lemmatize words. \"\"\"\n",
    "    return ' '.join([lemmatizer.lemmatize(word, **kwargs) for word in text.split()])\n",
    "\n",
    "text_processed = lemmatize_words(text_processed)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX1GEsTJA745"
   },
   "source": [
    "#### Cleaning HTML\n",
    "\n",
    "For a more sophisticated cleaning of text, you might want to consider **regular expressions**. In a nutshell, regular expressions are a family of text processing techniques for searching and replacing text. Their capability to match expressions in a text, for example an email, is quite powerful. A quick read through the corresponding [Wikipedia page](https://en.wikipedia.org/wiki/Regular_expression) would be useful. Also, here is a [nice playground](https://regexr.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1HvWJK1A746"
   },
   "source": [
    "Let's at least exemplify regular expression briefly. To that end, we need some new demo text, which includes HTML. Here we go:<br>\n",
    "This text includes the email address of Stefan, which is <stefan.lessmann@hu-berlin.de>. \n",
    "Also, we use <em>html</em> to <b>emphasize</b> parts and include breaks <br> to separate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1665573854905,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "yWF2XQHcA746"
   },
   "outputs": [],
   "source": [
    "# Another piece of demo text illustrating some common issues\n",
    "re_demo = \"\"\"\n",
    "            This text includes the email address of Stefan, which is <stefan.lessmann@hu-berlin.de>. \n",
    "            Also, we use <em>html</em> to <b>emphasize</b> parts and include breaks <br> to separate lines.\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm80_WmlA747"
   },
   "source": [
    "Finding or filtering email addresses is a common use case when processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1665573854906,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "iOsPzg9aA748",
    "outputId": "0808d9ae-8642-4d59-d829-560fac69d4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:  ['stefan.lessmann@hu-berlin.de']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This text includes the email address of Stefan, which is <>. Also, we use <em>html</em> to <b>emphasize</b> parts and include breaks <br> to separate lines.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding emails using RE\n",
    "import re  # Python library for regular expressions\n",
    "\n",
    "# Simple pattern to match email addresses\n",
    "pat = '([\\w\\.-]+@[\\w\\.-]+\\.[\\w]+)+'\n",
    "\n",
    "# Extracting email addresses\n",
    "email = re.findall(pat, re_demo)\n",
    "print('Found: ', email)\n",
    "\n",
    "# Filter sub-strings\n",
    "re.sub(pat, '', remove_whitespace(re_demo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euxf0TaJst0a"
   },
   "source": [
    "So we can filter e-mails. Nice. But creating such regular expressions for all sorts of HTML tags we might want to filter will prove challenging. Luckily, we do not have to worry. Entry `Beatiful Soup` <br>With no more than two lines of code, we our text is nice and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1665573855323,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "_HcSgtGQA749",
    "outputId": "dce93603-07f4-44fc-a3ae-8a9d02dcd6b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This text includes the email address of Stefan, which is . Also, we use html to emphasize parts and include breaks to separate lines.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library beatifulsoup4 handles html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove html content\n",
    "remove_whitespace(BeautifulSoup(re_demo).get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSn3xjDQVVGI"
   },
   "source": [
    "#### Conversion of Emojis and Emoticons\n",
    "\n",
    "Emoticons and emojis are a sequence of ASCII characters or unicode images that express moods or feelings in written communication. In use cases like sentiment analysis, emoticons and emojis give very valuable information.\n",
    "\n",
    "One way to make use of the information they may convey is to convert the emoticons and emojis into text that reflects their meaning. To that end, we will use the `emoji` package. When running this notebook in Google Colab, the easiest way to ready your environment for handling emojies and emoticons would be to just run `!pip install emoji`. The library is also available for conda. Make sure to have it installed prior to running the below code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1665574199562,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "pDJiwUde9CRT",
    "outputId": "5a509e79-28a8-4d46-c8c7-ccc7482ec90e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The movie was fantastic :o :-)) :rocket: :clapping_hands:'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "emo_demo = \"\"\"\n",
    "            The movie was fantastic :o :-)) 🚀 👏\n",
    "           \"\"\"\n",
    "\n",
    "remove_whitespace( emoji.demojize(emo_demo) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJqeXfmYjYfJ"
   },
   "source": [
    "Side remark, the package also supports the opposite operation. Take a look at the function `emojize()` if interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlDAJAcKA74-"
   },
   "source": [
    "# Wrapping up\n",
    "Albeit simple, the above demos provide a glance on text cleaning. While you could do a lot more, tasks like stop word removal, etc. will come up in many NLP projects. We conclude this part by putting all of the above steps into a helper function, which we will use later to clean a data set of online movie reviews. Our helper function will use lemmatization instead of stemming because it is likely to give better results in downstream tasks (i.e., text classification). The following function is a helper function to call the lemmatizer with the right dictionary form of a word.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1665574205561,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "zy-F92DLA75C"
   },
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Helper function that calls the POS tagger for an input word and return a code that can be used for lemmatization\"\"\"\n",
    "    # Extract the first letter of the POS tag (see the above example to understand the output coming from pos_tag)\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  \n",
    "    # Dictionary to map these letters to wordnet codes that the lemmatizer understands\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1665574209062,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "i8EcGqdoA75C",
    "outputId": "a972430b-2593-4bc4-d370-da47281a60d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'v', 'n', 'n', 'n', 'a', 'n']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function\n",
    "[get_wordnet_pos(x) for x in [\"She\", \"earned\", \"her\", \"stripes\", \"with\", \"great\", \"performance\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ljzZyj2A75D"
   },
   "source": [
    "And here is the real helper function for text cleaning. We will make use of it right after introducing our data set for subsequent parts. Since that data is stored in the form of a data frame, we refrain from making our helper function more general and simply assume that incoming text is a Pandas Series object (i.e., one column of a data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1665574212555,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "D1yH62ujlemE"
   },
   "outputs": [],
   "source": [
    "def text_cleaning(documents):\n",
    "    \"\"\"\n",
    "    Function for standard NLP pre-processing including removal of html tags,\n",
    "    whitespaces, non-alphanumeric characters, and stopwords. Emoticons are\n",
    "    converted to text that reflects their meaning. Words are subject to\n",
    "    lemmatization using their POS tags.\n",
    "    \"\"\"\n",
    "    cleaned_text = []  # our output will be a list of documents\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print('Processing input array with {} elements...'.format(documents.shape[0]))\n",
    "    counter = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        text = BeautifulSoup(doc).get_text() # remove html content\n",
    "        text = remove_whitespace(text) # remove whitespaces\n",
    "        text = emoji.demojize(text) # convert emoticons to text\n",
    "        text = remove_punctuation_and_casing(text) # remove punctuation and casing\n",
    "        text = remove_stopwords(text) # remove stopwords\n",
    "        text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()]) # lemmatize each word\n",
    "        \n",
    "        cleaned_text.append(text)\n",
    "\n",
    "        if (counter > 0 and counter % 50 == 0):\n",
    "            print('Processed {} documents'.format(counter))\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADG0dtq1A75E"
   },
   "source": [
    "## 2. Use case: the IMDB movie review data set\n",
    "We use a popular NLP data set consisting of movie reviews posted at [IMDB](https://www.imdb.com/). The data is available in different sizes and shapes (cleaned, raw, ...) on the web. We use a version from Kaggle, which includes 50K reviews and binary labels whether a review is positive or negative. The labels are useful for sentiment analysis, which we will do in a future demo. Here, we simply prepare the data for subsequent uses and, in doing so, further elaborate on the NLP operations introduced in the previous part. You can download the raw data from Kaggle: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data. A version is also available in our [GitHub repository](https://github.com/Humboldt-WI/adams/tree/master/demos/nlp). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z3LFVVCA75F"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 959,
     "status": "error",
     "timestamp": 1665574216705,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "W632-T2fA75F",
    "outputId": "7df00831-efa6-436c-dccf-71be11137c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remeber to adjust the path so that it matches your environment\n",
    "import pandas as pd\n",
    "\n",
    "imdb_data = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1665573855325,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "xa8Oiw5bA75G"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQGcjRNEA75G"
   },
   "source": [
    "The data is really simple; just two columns, one for the binary sentiment and one for the text of the review. Apparently, some of the reviews include HTML. We already added functionality to handle HTML into our text cleaning function. So this should not cause us any trouble. Let's look at an arbitrary review to get a better understanding of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1665573855325,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "nO7pNaz_A75H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 8\n",
    "imdb_data.loc[ix, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1665573855325,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "76meDV-dA75H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.loc[ix, 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FwuSSJjA75I"
   },
   "source": [
    "### Sampling\n",
    "Working with the full data set of 50K reviews is time consuming. When experimenting with the notebook, you might want to draw a random sample to increase the speed of computations. For a modern computer, a sample size of 5000 should be feasible without increasing the time too much. For the same of our demo, we use only 500 reviews to save time. \n",
    "Note that results of processing the full data sets are available in our course folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855326,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "iSZlYQpCA75I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     500 non-null    object\n",
      " 1   sentiment  500 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Draw a radnom sample to save time\n",
    "sample_size = 500\n",
    "np.random.seed(888)\n",
    "idx = np.random.randint(low=0, high=imdb_data.shape[0], size=sample_size)\n",
    "imdb_data = imdb_data.loc[idx,:]\n",
    "\n",
    "imdb_data.reset_index(inplace=True, drop=True)  # dropping the index prohibits a reidentification of the cases in the original data frame\n",
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYum-n2lA75K"
   },
   "source": [
    "### Data cleaning\n",
    "Thanks to our careful preparation, cleaning the reviews should be easy. All it takes is applying our cleaning function to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855326,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "I6lIb9J4rdnD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input array with 500 elements...\n",
      "Processed 50 documents\n",
      "Processed 100 documents\n",
      "Processed 150 documents\n",
      "Processed 200 documents\n",
      "Processed 250 documents\n",
      "Processed 300 documents\n",
      "Processed 350 documents\n",
      "Processed 400 documents\n",
      "Processed 450 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\anaconda3\\envs\\deepml\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 14 sec\n"
     ]
    }
   ],
   "source": [
    "# Do the cleaning\n",
    "# CAUTION: depending on your data set size, the processing might take a while \n",
    "import time  # To keep an eye on runtimes\n",
    "start = time.time()\n",
    "imdb_data['review_clean'] = text_cleaning(imdb_data.review)\n",
    "print('Duration: {:.0f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855326,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "kyrIcWSwA75M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      "i, too, loved this series when i was a kid. In 1952 i was 5 and my family always watched this show. My favorite character was the one played by Marion Lorne as a rather stuttering, bumbling and very lovable \"aunt\" type person. i can still recall her \"ubba bubba um um\" type comments as she would try and say something important. And then when she came back and played Aunt Clara in Bewitched it was great casting! <br /><br />It was the first time that i can remember seeing Walter Matthau whose career i followed as a fan for many many years.<br /><br />i have a question if anyone can verify: was the title or end credits music the \"Swedish Rhapsody\" by Hugo Alfven? Every time i hear it played on my classical radio station here in Southern California it brings back memories of the image of Mr. Peepers walking away with his back to the camera. i'm not even certain if this image in my mind's eye is correct.\n",
      "\n",
      "Cleaned Review:\n",
      "love series kid family always watch show favorite character one played marion lorne rather stutter bumbling lovable aunt type person still recall ubba bubba um um type comment would try say something important come back played aunt clara bewitch great cast first time remember see walter matthau whose career follow fan many many year question anyone verify title end credit music swedish rhapsody hugo alfven every time hear played classical radio station southern california brings back memory image mr peeper walk away back camera even certain image eye correct\n"
     ]
    }
   ],
   "source": [
    "# Check all is well\n",
    "ix = 0  # just one example, play with other play to further examine the effect of our clearning\n",
    "print('Original Review:\\n' + imdb_data.review[ix])  \n",
    "print('\\nCleaned Review:\\n' + imdb_data.review_clean[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855326,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "cAeDBflKvbm7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   review        500 non-null    object\n",
      " 1   sentiment     500 non-null    object\n",
      " 2   review_clean  500 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855327,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "mbL-YTjPr6pv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i, too, loved this series when i was a kid. In...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love series kid family always watch show favor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film on the same night I saw 6 othe...</td>\n",
       "      <td>positive</td>\n",
       "      <td>saw film night saw short one leap bound ahead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My first thoughts on this film were of using s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>first thought film use science fiction bad way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I couldn't tell if \"The Screaming Skull\" was t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>tell scream skull try hitchcock rip modernize ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I found The FBI Story considerably entertainin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>found fbi story considerably entertain suitabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Firstly, the title has no relevance whatsoever...</td>\n",
       "      <td>negative</td>\n",
       "      <td>firstly title relevance whatsoever movie start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The acronymic \"F.P.1\" stands for \"Floating Pla...</td>\n",
       "      <td>negative</td>\n",
       "      <td>acronymic f p stand float platform film porten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This short is one of the best of all time and ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>short one best time proof like charlie work so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  i, too, loved this series when i was a kid. In...  positive   \n",
       "1  I saw this film on the same night I saw 6 othe...  positive   \n",
       "2  My first thoughts on this film were of using s...  negative   \n",
       "3  I couldn't tell if \"The Screaming Skull\" was t...  negative   \n",
       "4  I found The FBI Story considerably entertainin...  positive   \n",
       "5  Firstly, the title has no relevance whatsoever...  negative   \n",
       "6  The acronymic \"F.P.1\" stands for \"Floating Pla...  negative   \n",
       "7  This short is one of the best of all time and ...  positive   \n",
       "\n",
       "                                        review_clean  \n",
       "0  love series kid family always watch show favor...  \n",
       "1  saw film night saw short one leap bound ahead ...  \n",
       "2  first thought film use science fiction bad way...  \n",
       "3  tell scream skull try hitchcock rip modernize ...  \n",
       "4  found fbi story considerably entertain suitabl...  \n",
       "5  firstly title relevance whatsoever movie start...  \n",
       "6  acronymic f p stand float platform film porten...  \n",
       "7  short one best time proof like charlie work so...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFkavtnmA75M"
   },
   "source": [
    "Looks like the cleaning has fulfilled its purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5_nwC3bA75O"
   },
   "source": [
    "### File input and output\n",
    "Should you have used the full data set in the above cleaning, you will want to store your results. The following codes exemplifies the use of a library called `Pickle`, which Pandas support natively to store data sets in a binary format. Compared to csv, the advantage of a binary format is that the data needs less space on disk. Note that you might have to install `Pickle` for the code to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855327,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "92n7hUs-A75Q"
   },
   "outputs": [],
   "source": [
    "# Saving objects to disk using pickle\n",
    "import pickle\n",
    "\n",
    "imdb_data.to_pickle('your_file_name.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV8EPJzWA75Q"
   },
   "source": [
    "### A bird's eye view on the data\n",
    "Let's have a quick look at what folks talk about in this data set. Using the class *Counter* from the collections package, we can easily count word occurrences and query the most common words. We can also check the number of occurrences for specific words. We do not really need the *word_counter* here and only use it to get a feeling for the data set. Our course, these types of checks make more sense when using the full data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1665573855327,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "dNPKVlhSA75R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a bit of code to load the full data set of the cleaned reviews, make sure to download it first\n",
    "import pickle\n",
    "with open('imdb_clean_full_v2.pkl','rb') as path_name:\n",
    "    clean_reviews = pickle.load(path_name)\n",
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1665573855328,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "2toK51cXA75R"
   },
   "outputs": [],
   "source": [
    "# Loop through the words and update a counter keeping track of word counts\n",
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for r in clean_reviews[\"review_clean\"]:\n",
    "    for w in r.split():        \n",
    "        word_counter.update({w: 1})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1665573855328,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "c9oiRYXCA75S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 102248),\n",
       " ('film', 93765),\n",
       " ('one', 54830),\n",
       " ('make', 46061),\n",
       " ('like', 44268),\n",
       " ('see', 41548),\n",
       " ('get', 34802),\n",
       " ('well', 32800),\n",
       " ('time', 31451),\n",
       " ('good', 29700)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the top most frequent words\n",
    "top_n = 10\n",
    "word_counter.most_common(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1dInKqlA75T"
   },
   "source": [
    "The above results hints at some more challenges when working with text data. Among the top ten most frequent words, none is really surprising or appears interesting. Well, what is interesting depends on the task. For example, words like *like* and *good* have meaning in a sentiment analysis setting. However, words like *movie* or *film* will naturally appear in a data set on movie reviews and will likely not contribute useful information to any downstream task. This indicates that, in addition to filtering stop words, there could be other 'normal' words (i.e., not stop words) that we might want to filter. Again, preparing text data can be rather laborious...<br>\n",
    "Let's check if people also talk about something more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1665573855328,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "u87DFzCDA75U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check frequency of some target word\n",
    "word_counter[\"spielberg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-tJvEHVst0g"
   },
   "source": [
    "## Summary\n",
    "Well done, you hit the end of yet another ADAMS demo exposing you to the fundamentals of text preprocessing and NLP. Our next demo will bring us back to the IMDB movie data sets and revisit a famous algorithm for learning distributed representations of textual data called **Word-to-Vec**. So stay tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1665573855329,
     "user": {
      "displayName": "Stefan Lessmann",
      "userId": "06342662613942148717"
     },
     "user_tz": -120
    },
    "id": "j1xTAjO3st0h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
