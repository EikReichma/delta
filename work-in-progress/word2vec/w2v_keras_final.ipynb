{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/demos/nlp/w2v_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gVyf-iazbBM",
    "tags": []
   },
   "source": [
    "# Word-to-Vec (W2V) in Keras\n",
    "This notebook is the third demo on natural language processing and word embeddings. Having discovered the [fundamentals of NLP](https://github.com/Humboldt-WI/adams/blob/master/demos/nlp/nlp_foundations.ipynb) and how we can train [custom word embeddings using the Gensim library](https://github.com/Humboldt-WI/adams/blob/master/demos/nlp/word-2-vec.ipynb), the purpose of this demo is to illustrate a from scratch implementation of the W2V algorithm. To that end, we will draw heavily on the very good Word-to-Vec tutorial on the [Tensorflow homepage](https://www.tensorflow.org/tutorials/text/word2vec). As shown there, we provide an implementation based on Keras. \n",
    "\n",
    "If you are interested, you can find many tutorials that walk you through a from scratch implementation of W2V using nothing but plain Python and Numpy. Here are some examples:\n",
    "- [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
    "- [Word2vec from Scratch](https://jaketae.github.io/study/word2vec/)\n",
    "- [Word2vec from Scratch with Python and NumPy](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/)\n",
    "\n",
    "Last, it goes without saying that excellent resources including various Python codes can be obtained from [Dive into Deep Learning](https://www.d2l.ai/), [Chapter 14](https://www.d2l.ai/chapter_natural-language-processing-applications/index.html).\n",
    "\n",
    "Now, without further delay, let's move on with our ADAMS demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gVyf-iazbBM",
    "tags": []
   },
   "source": [
    "## Recap W2V\n",
    "Remember that W2V proposes two models for learning word vectors, continuous-bag-of-words (CBOW) and Skip-Gram. In a nutshell, CBOW predicts a central target word from surrounding context words, while Skip-Gram takes the opposite approach. Given a <font color='red'>target word</font>, predict <font color='green'>context words</font> with high chance to appear next to the target word in a corpus. Considering one of the above example sentences and a widow size of 2, we can highlight target and context words as follows:<br><br>\n",
    "[doctors <font color='green'>claim the</font><font color='red'> air </font><font color='green'>you breath</font> defines]. \n",
    "<br><br>Using a question mark to indicate the target variable of the model, we obtain:\n",
    "\n",
    "[doctors *? ?* **air** *? ?* breath] in Skip-Gram versus [doctors *claim the* **?** *you breath* defines] in CBOW.\n",
    "\n",
    "In this section, we focus on Skip-Gram, which seems to be the preferred approach in practice. The code is based on a tutorial that is part of the [Tensorflow documentation](https://www.tensorflow.org/tutorials/text/word2vec) and hence optimized for keras 2.\n",
    "\n",
    "Before moving on, let's recall the architecture of the skip-gram W2V model.\n",
    "\n",
    "![sg](https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png)\n",
    "<br>\n",
    "Source: [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png)\n",
    "\n",
    "Given a sentence – better to say sequence of text – we take a target word and predict a set of context words, that is, words, which appear in a certain <font color=\"green\">**context window**</font>  $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, where $i$ is the *window size* and the number of context words to consider is window size $\\times 2$. \n",
    "\n",
    "An important caveat with the above picture is that a corresponding model would not scale. Remember that the output layer involves a high-dimensional softmax which is too costly to compute for any reasonably sized corpus. Among the two options around this problem, *hierarchical softmax* and *negative sampling*, we will make use of the latter. So given a target word, our prediction task will be to classify whether another word is an actual context word for that target word, or a random word sampled from the corpus according to some probability distribution. This is a binary classification task. Thus, the output of our neural network is much cheaper to compute. Instead of a high-dimensional softmax we only need a simple logistic classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1658774710852,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "YudKf_R_f0bC"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9C8DqQy9KGV"
   },
   "source": [
    "## Importing the IMDB movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24650,
     "status": "ok",
     "timestamp": 1658774735495,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "tnwnAC9iCf-1",
    "outputId": "54b90c84-496c-4b0c-e635-0387dbb81dfc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Configure variables pointing to directories and stored files \n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # mount Google-Drive\n",
    "    directory = '/content/drive/My Drive/ADAMS/'  # adjust to Google drive folder with the data if applicable\n",
    "else:\n",
    "    directory = \"../NLP/\" # adjust to the directory where data is stored on your machine (if running the notebook locally)\n",
    "\n",
    "sys.path.append(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldjobNLXfSr8"
   },
   "source": [
    "When running this notebook in Google Colab, ensure that you run it with a GPU as hardware accelerator. To enable this:\n",
    "- Navigate to Edit → Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1658774735498,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "eRP-FEizYHA6"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4159,
     "status": "ok",
     "timestamp": 1658774739640,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "Lh1eZAADzbBN"
   },
   "outputs": [],
   "source": [
    "# We use our cleaned IMDB data set for the demo\n",
    "with open(directory + 'imdb_clean_full_v2.pkl','rb') as path_name:\n",
    "    df_imdb = pickle.load(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1658774739646,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "a6KiwA8oDLY5",
    "outputId": "44574e67-b4dd-4b63-cd37-ebaf7ce27073"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mention watch oz episode hooked r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter love time money visually stun film watc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                        review_clean  \n",
       "0  one reviewer mention watch oz episode hooked r...  \n",
       "1  wonderful little production film technique una...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter love time money visually stun film watc...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAViJJKAzbBN"
   },
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "Let's start with building our vocabulary. It is common practice to not train on every word but words that occur reasonably frequent. For rare words, training a good embedding is difficult. Remember how this issue motivated subword embeddings like Fasttext. In our example, we simply use the most frequent words from the review corpus and try to compute embeddings for these words. This is the point where our word_counter comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1658774747094,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "uM8BAfXVDF1E"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13096,
     "status": "ok",
     "timestamp": 1658774763073,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "hAlTjLFNzbBN",
    "outputId": "df51d166-66dc-44fb-8d63-5d06366b484e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie', 'film', 'one', 'make', 'like', 'see', 'get', 'well', 'time', 'good']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is copied from the NLP foundations notebook. \n",
    "word_counter = Counter()\n",
    "for r in df_imdb['review_clean']:\n",
    "    for w in r.split():  # this is like tokenizing using the white space\n",
    "        word_counter.update({w: 1})\n",
    "\n",
    "# Extract the n most common words from the corpus\n",
    "#vocab_size = 4000\n",
    "vocab_size = 1000  # run the code with a vocab size of only 1000 to speed up the process\n",
    "vocab = word_counter.most_common(vocab_size)\n",
    "vocab = [x[0] for x in vocab]\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i95eLnSNzbBO"
   },
   "source": [
    "Next task is to build a dictionary. For Keras, we need to encode words as integers, which Keras will then interpret as indices into a one-hot vector of the size of the vocabulary. We build two dictionaries. One to map words to their code (i.e. unique integer) and one to revert the mapping and decode words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1658774763074,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "mp-2hCTZzbBO"
   },
   "outputs": [],
   "source": [
    "idx = range(1, vocab_size)\n",
    "word2id = dict(zip(vocab, idx))\n",
    "id2word = dict(zip(idx, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1658774763074,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "FQkb3QunzbBO",
    "outputId": "f0279c55-613f-4f88-fa18-d48427b5da60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1000\n",
      "Vocabulary Sample: [('movie', 1), ('film', 2), ('one', 3), ('make', 4), ('like', 5), ('see', 6), ('get', 7), ('well', 8), ('time', 9), ('good', 10)]\n",
      "[('mood', 990), ('regard', 991), ('jane', 992), ('garbage', 993), ('reference', 994), ('barely', 995), ('haunt', 996), ('super', 997), ('humour', 998), ('impressive', 999)]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: {}'.format(vocab_size))\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n",
    "print(list(word2id.items())[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M2isCo3zbBQ"
   },
   "source": [
    "You may have noted that we have so far left out the index 0. This index is commonly reserved for unknown words, which we map to a special token. Remember that our vocabulary is not very large when compared to the number of words that exists in a language (e.g. ~ 300k in English). So when processing texts, we will run into a lot of unknown words. We deal with these words by mapping them to the token `UNK`. This way, we learn one embedding for all unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1658774767486,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "wKxx6wHWzbBQ"
   },
   "outputs": [],
   "source": [
    "word2id['UNK'] = 0\n",
    "id2word[0] = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFELOQoJzbBR"
   },
   "source": [
    "Now we are ready to turn our reviews into integer numbers, which is the format that Keras expects, while accounting for unknown words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1658774767982,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "c-4WxtFuzbBQ"
   },
   "outputs": [],
   "source": [
    "# Build the corpus for W2V by encoding the reviews\n",
    "def encode_review(review, dictionary):\n",
    "    output = []\n",
    "    for word in review:\n",
    "        if word not in dictionary.keys():\n",
    "            output.append(dictionary['UNK'])\n",
    "        else:\n",
    "            output.append(dictionary[word])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 2459,
     "status": "ok",
     "timestamp": 1658774773320,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "wguSwoGUzbBR"
   },
   "outputs": [],
   "source": [
    "coded_reviews = []\n",
    "for r in df_imdb['review_clean']:\n",
    "    coded_reviews.append(encode_review(r.split(), word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1658775338905,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "GnzbtPXVzbBR",
    "outputId": "31993d1f-a500-4c94-c536-80477b5a9cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encourage positive comment film look forward watch film bad mistake see film truly one bad awful almost every way edit pace storyline soundtrack song lame country tune played less four time film look cheap nasty boring extreme rarely happy see end credit film thing prevents give score harvey keitel far best performance least seem make bit effort one keitel obsessive\n",
      "[0, 928, 306, 2, 22, 687, 11, 2, 14, 840, 6, 2, 276, 3, 14, 280, 123, 85, 35, 432, 418, 614, 583, 257, 677, 442, 0, 162, 251, 519, 9, 2, 22, 556, 0, 267, 0, 0, 413, 6, 25, 379, 2, 37, 0, 32, 385, 0, 0, 134, 49, 65, 129, 40, 4, 107, 453, 3, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Some testing\n",
    "id_demo_review = 8  # one random review\n",
    "demo_review = df_imdb['review_clean'][id_demo_review]\n",
    "print(demo_review)  # plain text after cleaning\n",
    "\n",
    "# One-hot-coding representation in which integer numbers represent the\n",
    "# index of the single non-zero element in a one-hot-vector of dimensionality \n",
    "# vocab_size\n",
    "print(coded_reviews[id_demo_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 928, 306, 531]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One more example: compare the id's of words in this demo text with those from the output of encoding review no. 8 above\n",
    "demo_txt = [\"movie\", \"positive\", \"comment\", \"silly\"]\n",
    "encode_review(demo_txt, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1658775339944,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "Js0UmDOjzbBS",
    "outputId": "de21570d-8ae8-4f01-ef07-2331e0758231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good\n"
     ]
    }
   ],
   "source": [
    "if len(coded_reviews[id_demo_review]) == len(demo_review.split()):\n",
    "    print('Looks good')\n",
    "else:\n",
    "    raise ValueError('This can\\'t be right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGEfk5FtzbBS",
    "tags": []
   },
   "source": [
    "## Generating training data\n",
    "\n",
    "The training data for our skip-gram model consists of tuples (target, context) with corresponding label (0/1), indicating whether the second word really appeared in the context of the target word or not. Fortunately, Keras has ready-made functions that we can use to generate that training data. Below you can find an overview of the process from the tensorflow documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnUwPO7-p4-v"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://tensorflow.org/tutorials/text/images/word2vec_negative_sampling.png\" width=\"650\"/>\n",
    "</div>\n",
    "\n",
    "Source: [Tensorflow Documentation](https://tensorflow.org/tutorials/text/images/word2vec_negative_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDecxslPzbBS"
   },
   "source": [
    "Let's first illustrate the function `skipgrams()` for a single short sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2598,
     "status": "ok",
     "timestamp": 1658775342528,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "PJoPuhjn7bf-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1658775342529,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "2sIAszOtzbBS",
    "outputId": "cd821fb5-1827-4cb1-bb70-2c9459bfba64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read book forget movie\n",
      "[234, 144, 626, 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's pick a random review (you can use any text)\n",
    "text = df_imdb['review_clean'][27521]\n",
    "print(text)\n",
    "\n",
    "# Encoded version\n",
    "encoded_text = coded_reviews[27521]\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLpIrvwLzbBT"
   },
   "source": [
    "Remember that a window size of `i` translates to $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, so the number of context words to consider is window size $\\times 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1658775342529,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "N3YwY8RZzbBU",
    "outputId": "32bc5b21-6ed4-4b1e-d303-9ed9e41531ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(forget (626), movie (1))\n",
      "(read (234), book (144))\n",
      "(read (234), forget (626))\n",
      "(book (144), movie (1))\n",
      "(book (144), read (234))\n",
      "(forget (626), book (144))\n",
      "(book (144), forget (626))\n",
      "(movie (1), forget (626))\n",
      "(movie (1), book (144))\n",
      "(forget (626), read (234))\n"
     ]
    }
   ],
   "source": [
    "positive_skip_grams, _ = skipgrams(encoded_text,\n",
    "                         vocabulary_size=vocab_size,\n",
    "                         window_size=2,\n",
    "                         negative_samples=0)\n",
    "\n",
    "for i in range(len(positive_skip_grams)):\n",
    "    print('({:s} ({:d}), {:s} ({:d}))'.format(id2word[positive_skip_grams[i][0]], \n",
    "                                              positive_skip_grams[i][0], \n",
    "                                              id2word[positive_skip_grams[i][1]],\n",
    "                                              positive_skip_grams[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORPIH-16zbBU"
   },
   "source": [
    "What we get is a list of positive skipgrams. Next we will have to sample negative skipgrams, i.e. those that don't appear in the context window. According to the empirical evidence, the probability of a word to be sampled as a negative example should be relative to its frequency. Otherwise, we might end up focussing too much on the most frequent words. Keras provides the utility function `make_sampling_table` to calculate sampling weights for each word in the corpus. Details are available in the [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/make_sampling_table). The sampling table is a list of sampling probabilities, one for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1658775342531,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "Dcizrmw8zbBU",
    "outputId": "6851eadb-a5f3-4fbe-8caa-2fcaeac2403a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00315225, 0.00315225, 0.00547597, 0.00741556, 0.00912817,\n",
       "       0.01068435, 0.01212381, 0.01347162, 0.01474487, 0.0159558 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import make_sampling_table\n",
    "sampling_table = make_sampling_table(vocab_size)\n",
    "sampling_table[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsU8ePH6zbBV"
   },
   "source": [
    "Note the increasing magnitude of the sampling weights. Sampling words from the corpus using this sampling distribution requires that the words in the corpus are ordered by frequency. The idea is that when sampling negative examples we do not want to focus too much on the frequent words – in our case words like 'movie', 'film', and 'like'. We therefore raise the chance of less frequent words to be sampled as negative examples in accordance to their rank.\n",
    "\n",
    "Let's now put everything together and create a helper function that will take care of generating the training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1658775342532,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "IKhN8hSm9sCg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1658775342532,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "GpDT6JXZgv4p"
   },
   "outputs": [],
   "source": [
    "# Set the window size\n",
    "window_size = 2\n",
    "# Set the number of negative samples per positive context\n",
    "num_ns = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1658775342532,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "Ogj8V6EBiTPf"
   },
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    ''' This function generates skip-gram pairs with negative sampling for a list\n",
    "    of sequences (int-encoded sentences) based on window size, number of negative\n",
    "    samples and vocabulary size. '''\n",
    "\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm(sequences):\n",
    "\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype='int64'), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name='negative_sampling')\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "                negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat(\n",
    "                [context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1]+[0]*num_ns, dtype='int64')\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Transform the lists into numpy arrays\n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)[:, :, 0]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800387,
     "status": "ok",
     "timestamp": 1658776144550,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "es0GLJqS4E-M",
    "outputId": "7cf24d9c-586e-4497-d716-02f21372ec05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 50000/50000 [02:00<00:00, 414.21it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=coded_reviews,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY3jp4ZnzbBY",
    "tags": []
   },
   "source": [
    "## Building the neural network\n",
    "\n",
    "We are ready to design our NN architecture using Keras. We feed the network with pairs of target words and actual/fake context words. Each word is put through an embedding layer. Remember that W2V trains two embeddings per word, one when the word is the target word and one when the word appears in the context of some other target word. So using two embedding layers is important.\n",
    "\n",
    "Having obtained word embeddings for the target and context word, we pass these embeddings to a merge layer in which we compute the dot product of these two vectors. We can think of the dot products as an unnormalized cosine similarity between the two embedding vectors. Put differently, we obtain a similarity score. We want that score to be large when the inputted 'context' word actually appeared in the context of the target word, and small otherwise. Hence, we forward the similarity score to a dense sigmoid layer, which computes a probability of the 'context' word being an actual context word. We then compare this probability, the output of our neural network, to the actual label, which we obtained above from `generate_training_data` function. Enter back-propagation. \n",
    "\n",
    "So far so good, but there is one issue. Our network is a little more advanced than those we have built so far. There were also some changes when moving to Keras 2, which hit us in this example. Long story short, we cannot use the nice and simple sequential API anymore and will have to use the functional API instead. For this reason, the code will look a little different from what you are used to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1658776144552,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "dQsMlbnQ4Jq6"
   },
   "outputs": [],
   "source": [
    "# Create tensorflow dataset from the generated training data (this includes\n",
    "# randomizing the data, creating batches and speeding up the performance by\n",
    "# enabling caching and prefetching)\n",
    "dataset = (tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "           .shuffle(10000)\n",
    "           .batch(1024, drop_remainder=True)\n",
    "           .cache()\n",
    "           .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1658776144553,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "PhYbamG44fTV"
   },
   "outputs": [],
   "source": [
    "# Define the structure of the keras model\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  \n",
    "    # Define layers in __init__ function\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super().__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                                 embedding_dim,\n",
    "                                                 input_length=1,\n",
    "                                                 name='target_embedding')\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                                  embedding_dim,\n",
    "                                                  input_length=num_ns+1,\n",
    "                                                  name='context_embedding')\n",
    "        # 5 dimensions in the dense layer, because we have one positive and 4\n",
    "        # negative context words\n",
    "        self.dense = layers.Dense(5, activation='sigmoid')\n",
    "\n",
    "    # Implement forward pass in call function\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dot_product = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        output = self.dense(dot_product)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1658776144554,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "CFNAyhG64lT8"
   },
   "outputs": [],
   "source": [
    "# Initiate the model and configure it with an optimizer, loss and metrics\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim, num_ns)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92697,
     "status": "ok",
     "timestamp": 1658776237232,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "rOd4N2K04_B9",
    "outputId": "b9be4859-e492-44bf-d143-5bb2fbf22ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "970/970 [==============================] - 6s 5ms/step - loss: 0.1659 - accuracy: 0.9939\n",
      "Epoch 2/20\n",
      "970/970 [==============================] - 3s 3ms/step - loss: 2.3135e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 6.8536e-05 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "970/970 [==============================] - 3s 3ms/step - loss: 2.9107e-05 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 1.4297e-05 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 7.5747e-06 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 4.2003e-06 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 2.4032e-06 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 1.4091e-06 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 8.4309e-07 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 5.1346e-07 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "970/970 [==============================] - 3s 4ms/step - loss: 3.1771e-07 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 1.9946e-07 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 1.2663e-07 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 8.0590e-08 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 5.1046e-08 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 3.1828e-08 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 1.9346e-08 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 1.1376e-08 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "970/970 [==============================] - 4s 4ms/step - loss: 6.4268e-09 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb7b3083a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now fit the model while iterating over the dataset 20 times\n",
    "word2vec.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDQ-sZUHEBgB"
   },
   "source": [
    "See how the model is not much of a neural network? The overwhelming majority of the trainable parameters are the embeddings, which are then dot-multiplied. We thus have two hidden layers side-by-side rather than one after the other and no non-linear activation of the hidden layers! This is very similar to matrix factorization and you can use the same architecture to build a collaborative filter on users (one embedding matrix) and items (one embedding matrix); just in case you are into recommender engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1658776237235,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "Tyc0ZNPrPoKw",
    "outputId": "beefbca2-1346-431b-eda3-4128ca074b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " target_embedding (Embedding  multiple                 128000    \n",
      " )                                                               \n",
      "                                                                 \n",
      " context_embedding (Embeddin  multiple                 128000    \n",
      " g)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,030\n",
      "Trainable params: 256,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word2vec.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-kglbMDzbBa"
   },
   "source": [
    "Here is a perhaps more intuitive visualization of the model:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1123/1*4Uil1zWWF5-jlt-FnRJgAQ.png\">\n",
    "\n",
    "Source: [Medium](https://miro.medium.com/max/1123/1*4Uil1zWWF5-jlt-FnRJgAQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZBiVZfhzbBc",
    "tags": []
   },
   "source": [
    "## Extracting the weights\n",
    "\n",
    "We can extract the word embeddings from the target embedding layer of our model  using `Model.get_layer` and `Layer.get_weights`. Converting the embeddings to a dataframe facilitates a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1658776237240,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "TX9QzKJNrO6m",
    "outputId": "2a88887a-cb32-4c86-e286-cc33c124c0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>0.007403</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>0.039849</td>\n",
       "      <td>-0.005933</td>\n",
       "      <td>-0.043833</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>0.019333</td>\n",
       "      <td>-0.041979</td>\n",
       "      <td>-0.009944</td>\n",
       "      <td>-0.001832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019483</td>\n",
       "      <td>0.044691</td>\n",
       "      <td>-0.015024</td>\n",
       "      <td>0.047083</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.024771</td>\n",
       "      <td>-0.007073</td>\n",
       "      <td>-0.014015</td>\n",
       "      <td>-0.007731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>0.127619</td>\n",
       "      <td>0.146523</td>\n",
       "      <td>-0.105528</td>\n",
       "      <td>-0.156217</td>\n",
       "      <td>0.192956</td>\n",
       "      <td>0.189649</td>\n",
       "      <td>-0.102191</td>\n",
       "      <td>0.128531</td>\n",
       "      <td>-0.142733</td>\n",
       "      <td>0.173058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>-0.139883</td>\n",
       "      <td>0.158247</td>\n",
       "      <td>-0.110454</td>\n",
       "      <td>0.180473</td>\n",
       "      <td>0.097302</td>\n",
       "      <td>0.106072</td>\n",
       "      <td>0.114954</td>\n",
       "      <td>0.145535</td>\n",
       "      <td>0.128355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.145414</td>\n",
       "      <td>0.086958</td>\n",
       "      <td>-0.155354</td>\n",
       "      <td>-0.154791</td>\n",
       "      <td>0.184328</td>\n",
       "      <td>0.169703</td>\n",
       "      <td>-0.148181</td>\n",
       "      <td>0.158294</td>\n",
       "      <td>-0.169188</td>\n",
       "      <td>0.149171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047883</td>\n",
       "      <td>-0.125324</td>\n",
       "      <td>0.092685</td>\n",
       "      <td>-0.156749</td>\n",
       "      <td>0.166181</td>\n",
       "      <td>0.155183</td>\n",
       "      <td>0.147127</td>\n",
       "      <td>0.195911</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.113477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.067413</td>\n",
       "      <td>-0.146721</td>\n",
       "      <td>-0.101469</td>\n",
       "      <td>0.144049</td>\n",
       "      <td>0.180389</td>\n",
       "      <td>-0.185780</td>\n",
       "      <td>0.169884</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>0.103540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-0.192230</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>-0.189832</td>\n",
       "      <td>0.218721</td>\n",
       "      <td>0.132661</td>\n",
       "      <td>0.108176</td>\n",
       "      <td>0.140849</td>\n",
       "      <td>0.199588</td>\n",
       "      <td>0.081444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.114674</td>\n",
       "      <td>0.118794</td>\n",
       "      <td>-0.163889</td>\n",
       "      <td>-0.181996</td>\n",
       "      <td>0.181742</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>-0.160769</td>\n",
       "      <td>0.137906</td>\n",
       "      <td>-0.106992</td>\n",
       "      <td>0.185597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072031</td>\n",
       "      <td>-0.136441</td>\n",
       "      <td>0.148543</td>\n",
       "      <td>-0.194120</td>\n",
       "      <td>0.169525</td>\n",
       "      <td>0.155139</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.160898</td>\n",
       "      <td>0.149344</td>\n",
       "      <td>0.183718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "movie  0.007403 -0.013269  0.039849 -0.005933 -0.043833  0.015909  0.019333   \n",
       "film   0.127619  0.146523 -0.105528 -0.156217  0.192956  0.189649 -0.102191   \n",
       "one    0.145414  0.086958 -0.155354 -0.154791  0.184328  0.169703 -0.148181   \n",
       "make   0.170400  0.067413 -0.146721 -0.101469  0.144049  0.180389 -0.185780   \n",
       "like   0.114674  0.118794 -0.163889 -0.181996  0.181742  0.164045 -0.160769   \n",
       "\n",
       "            7         8         9    ...       118       119       120  \\\n",
       "movie -0.041979 -0.009944 -0.001832  ... -0.019483  0.044691 -0.015024   \n",
       "film   0.128531 -0.142733  0.173058  ...  0.010634 -0.139883  0.158247   \n",
       "one    0.158294 -0.169188  0.149171  ...  0.047883 -0.125324  0.092685   \n",
       "make   0.169884 -0.153277  0.103540  ...  0.000444 -0.192230  0.084499   \n",
       "like   0.137906 -0.106992  0.185597  ...  0.072031 -0.136441  0.148543   \n",
       "\n",
       "            121       122       123       124       125       126       127  \n",
       "movie  0.047083  0.040798  0.006431  0.024771 -0.007073 -0.014015 -0.007731  \n",
       "film  -0.110454  0.180473  0.097302  0.106072  0.114954  0.145535  0.128355  \n",
       "one   -0.156749  0.166181  0.155183  0.147127  0.195911  0.144122  0.113477  \n",
       "make  -0.189832  0.218721  0.132661  0.108176  0.140849  0.199588  0.081444  \n",
       "like  -0.194120  0.169525  0.155139  0.144008  0.160898  0.149344  0.183718  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = word2vec.get_layer('target_embedding').get_weights()[0]\n",
    "print(word_embeddings.shape)\n",
    "w2v_df = pd.DataFrame(word_embeddings, index=id2word.values())\n",
    "w2v_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0wuSla5HWPa"
   },
   "source": [
    "Now that we have the word embeddings we can calculate how similar words are to each other. We use some scikit-learn functionality to create a matrix of pairwise distances between words. We can then query the most similar words to some seed-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1658442619111,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "XI5WkK5XEsEe",
    "outputId": "c3e0a458-28f6-4251-ccaf-b9a6e38503cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_matrix.shape: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(w2v_df)\n",
    "print(f'distance_matrix.shape: {distance_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1658317823157,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "ZLzGh8q4Dkfu",
    "outputId": "63b9b03c-a334-4069-9d38-d0e2ef5c61cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': ['us', 'anti', 'hole', 'disappoint', 'convincing'],\n",
       " 'bad': ['three', 'head', 'wish', 'attack', 'always'],\n",
       " 'good': ['might', 'right', 'say', 'future', 'home']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the results will not (yet) make much sense if you trained on a small corpus\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['movie', 'bad', 'good']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMrExUTSPvaflunnu2m2HNU",
   "collapsed_sections": [],
   "name": "w2v_keras_final.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
